\chapter{Background overview}

\section{History}

The idea of using RL agents to build neural networks is not new, however, there are not so many research projects nowadays. Mostly the reason for this is that most of the research is held by business, and business usually is not optimistic about RL in production.

However, some good progress was made in recent years. In 2015, ResNet becomes a winner of ILSVRC 2015 in image classification, detection, and localization and winner of MS COCO 2015 detection and segmentation. This enormous network contained 152 layers optimized by a lot of professional engineers manually. This process is expensive in terms of time and resources. Image classification contests are constantly showing a growing amount of layers for best-performing networks (AlexNet, 2012 - 8 layers, GoogleNet, 2014 - 22 layers). Resnet has 1.7 million parameters. Each competition is turning researchers more and more towards automation of this work - and this is a place where NAS becomes a new trend.

Barret Zoph and Quoc Le. in \cite{ZophL16} used a recurrent network to generate the model descriptions of neural networks and train this RNN via RL agent to maximize the expected accuracy of the generated architectures on a validation set. This paper is one the most cited in this field and our research is heavily based on it.

In 2019, Google researchers developed a family of models, called EfficientNets, which surpass state-of-the-art accuracy with up to 10x better efficiency (smaller and faster) using AutoML - see \cite{2019arXiv190511946T}.

Amazon has two AutoML products to offer - Amazon SageMaker Autopilot for the creation of the classification and regression machine learning models and Amazon DeepAR for forecasting scalar (one-dimensional) time series using RNN. This paper is also heavily based on the probabilistic approach used in DeepAR because of itâ€™s spectacular results.

This section would not be full without the paper which shares a lot in common with this project. RL agent (Q-Learning, epsilon-greedy exploration rate control, finite state space) described in the paper outperformed meta-modeling approaches for network design on image classification tasks \cite{Baker2016DesigningNN}.

The interest to the topic becomes even hotter when the NAS benchmark and dataset was introduced by Google Research team \cite{pmlr-v97-ying19a}

A variety of methods have been proposed to perform NAS, including reinforcement learning, Bayesian optimization with a Gaussian process model, sequential model-based optimization (SMAC), evolutionary search, and gradient descent over the past few years. We see a lot of research potential in this field and we share a big passion for RL paradigm - and that's why this project exists.

\section{Reinforcement Learning}

RL is an machine learning paradigm often used when the exact mathematical model is unknown and data is unlabeled. In this project we would mainly concentrate on the Q Learning approach. We require having observable environment where software agent would be able to take actions which would lead to reward. Agent is designed in the way it should maximize reward by utilizing existing knowledge (exploitation) or exploring random actions (exploration). Algorithm needs to learn a policy which determines which action should be taken next given current state (or set of recent states). The environment agent is operating with is typically a Markov Decision Process.

\subsection{Markov Decision Process}

MDB is a generalization of mathematical framework which allows performing a sequence of actions in an environment where future states would depend on current states and yield a partly random outcomes.

In other words, Markov Decision Process is a description of set of actions agents should take to receive some reward. It is described by:
\begin{itemize}
  \item State (or states) $\sset$ that are fully observable by agent
  \item Set of actions $\aset$ which agent could take in given state
  \item Transition model $\Tdef$ - an effect in state accused by action taken by agent
  \item A reward function $\Rfun$ which defines a reward received by agent for taking this action
  \item A discount factor $\Ddef$ which is used to value a reward that could be received in future
  \item A policy $\pdef$ agent should learn
\end{itemize}

A policy which allows agent to maximize it's reward is called a solution of given MDP. Notation of MDP used above is taken from \cite{Thomas15a}.
\subsection{Exploration and exploitation dilemma}
To demonstrate exploration over exploitation problem we usually often refer to Multiarmed Bandit Problem.

Multiarmed bandit problem is MDP with stochastic reward. Imagine having N machines with reward probabilities $Q_1 .. Q_n$. Playing k-th bandit could result into reward of 1 with a possibility of $Q_k$ or into reward 0 with possibility of $1 - Q_k$. Real reward probabilities are unknown to the agent. Also, resources are limited, so, exploring them using law of large numbers is inefficient and impossible in terms of this problem.

The goal of agent playing multiarmed bandits is maximization of cummulative reward.

Simply saying, by playing known action (performing exploitation) agent may miss better option. This becomes a loss function of such algorithms - a regret player might by not selecting the optimal action. If the agent always plays random actions (exploration) it will be completely useless and would yield no good results.
In other words, to balance exploration and exploitation problem we would need to ensure that the agent is not overfitting to known actions but still uses gained knowledge. It often happens, that RL agents observes some \textbf{good} \textbf{enough} action and starts to use it constantly. This is obviously not the way we generally want an RL algorithm to behave.
\subsection{Epsilon-greedy approach}
Epsilon-greedy approach to solve exploration over exploitation dilemma is based on the idea, where agent would generally take best actions, but at some time iterations t it would explore a random actions. It often assumes that at the beginning of the training number of random actions would be bigger (to ensure that agent would be able to explore good actions at the very beginning) and later it would decay (to ensure that agent is actually playing best actions). Expected reward is often represented as a mean of previously received rewards when playing this action. 

\subsection{Upper-confidence bound}

UCB is another instrument to solve exploration over exploitation dilemma. General approach is similar to epsilon-greedy algorithm, however, UCB allows to use unexplored actions in favor of actions which algorithm is very certain about being bad ones.

UCB measures potential of action using upper confidence bound of the reward value in such a way that larger number of trials of certain action should give smaller bound. This also prevents algorithm from overfitting.

In this work we would refer to Theorem 1 from \cite{Auer2002} using a simple UCB approach which allows to receive logarithmic regret uniformly and without any preliminary knowledge about the reward distributions.

\[
U(a) = Q(a) + \sqrt{\frac{2 log n_a}{N}}
\]

Where Q(a) is expected reward from action a (generally a mean of rewards received by playing that action), $n_a$ is number of times this action was played and N is total number of plays.
\subsection{Deep Q Learning}
While Q learning is a algorithm which allows agent to learn an effective policy for maximizing cumulative reward in general MDP, deep Q learning is a same, model-free approach that uses Deep Neural Network to approximate the values of expected reward.

In recent years a lot of research is done in this field, even though Deep Q learning is treated as unstable solution because a slight change in input can change outputs significantly.

However, this approach has a proven success story. This work is inspired by \cite{MnihKSGAWR13} which was one of the first successful aprroaches to use deep neural network as a backend for reinforcement learning agent.

In general Deep Q learning algorithms stores experienced rewards in replay memory, from where they are later batched and used as input to the underlying neural network. That's usually the reason why a lot of research uses RNNs with LSTM (Long-Short Term Memory) cells as a backend.

This work uses a convolutional neural network as a backend for RL algorithm.
\section{Image classification problem}

Image classification problem is a machine learning problem aiming to recognize a visual concept of given image and assign some class (label) to it as an output.

Data-driven approach for this problem required to train algorithm on loads of images in order to understand which features images from one class have in common to other images of this class and different from images of other classes.

Naturally, at the beginning KNN (K Nearest Neighbors) and other simple clusterization algorithms were used.

However, the real move forward started when backpropagation techiques were applied to neural network architectures, which opened a way for a deep learning techiques. As Yann LeCun released LeNet-5  (see \cite{lecun-89c}) modern convolutional architectures started to develop rapidly.

There are several popular datasets such as MNIST, Flowers, ImageNet etc. which are constantly used in image classification contests. Modern deep neural networks outperform human-level accuracy on those datasets.

\section{Convolutional Neural Networks}
\section{Gaussian Distribution}



