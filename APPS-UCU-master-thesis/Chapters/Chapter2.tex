\chapter{Background overview}

The idea of using reinforcement learning agents to build neural networks is not new, however, there are not so many research projects nowadays. Mostly the reason of this is that most of research is hold by business, and business usually is not optimistic about Reinforcement Learning in production.

However, some good progress was made for recent years. In 2015, ResNet become a winner of ILSVRC 2015 in image classification, detection, and localization and winner of MS COCO 2015 detection, and segmentation. This enormous network contained of 152 layers optimized by a lot of professional engineers manually. This process is obviously expensive in terms of time and other resources. Image classification contests are constantly showing a growing amount of layers for best-performing networks (AlexNet, 2012 - 8 layers, GoogleNet, 2014 - 22 layers). Resnet has 1.7 million parameters. Each competition is turning researchers more and more towards automation of this work - and this is a place where neural architecture search becomes a new trend.

Barret Zoph and Quoc Le. in “Neural architecture search with reinforcement learning.” used a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. This paper is one the most cited in this field and our research is heavily based on it.

In 2019, Google researchers developed a family of models, called EfficientNets, which superpass state-of-the-art accuracy with up to 10x better efficiency (smaller and faster) using AutoML.

Amazon have two AutoML products to offer - Amazon SageMaker Autopilot for creation of the classification and regression machine learning models and Amazon DeepAR for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNN). This paper is also heavily based on probabilistic approach used in DeepAR because of it’s spectacular results.

A variety of methods have been proposed to perform NAS, including reinforcement learning, Bayesian optimization with a Gaussian process model, evolutionary search, and gradient descent over a past few years.