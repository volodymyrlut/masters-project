{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from controller import Controller, StateSpace\n",
    "from manager import NetworkManager\n",
    "from model import model_fn\n",
    "\n",
    "\n",
    "# create a shared session between Keras and Tensorflow\n",
    "policy_sess = tf.Session()\n",
    "K.set_session(policy_sess)\n",
    "\n",
    "NUM_LAYERS = 4  # number of layers of the state space\n",
    "MAX_TRIALS = 250  # maximum number of models generated\n",
    "\n",
    "MAX_EPOCHS = 10  # maximum number of epochs to train\n",
    "CHILD_BATCHSIZE = 128  # batchsize of the child models\n",
    "EXPLORATION = 0.8  # high exploration for the first 1000 steps\n",
    "REGULARIZATION = 1e-3  # regularization strength\n",
    "CONTROLLER_CELLS = 32  # number of cells in RNN controller\n",
    "EMBEDDING_DIM = 20  # dimension of the embeddings for each state\n",
    "ACCURACY_BETA = 0.8  # beta value for the moving average of the accuracy\n",
    "CLIP_REWARDS = 0.0  # clip rewards in the [-0.05, 0.05] range\n",
    "RESTORE_CONTROLLER = True  # restore controller to continue training\n",
    "\n",
    "# construct a state space\n",
    "state_space = StateSpace()\n",
    "\n",
    "# add states\n",
    "state_space.add_state(name='kernel', values=[1, 3])\n",
    "state_space.add_state(name='filters', values=[16, 32, 64])\n",
    "\n",
    "# print the state space being searched\n",
    "state_space.print_state_space()\n",
    "\n",
    "# prepare the training data for the NetworkManager\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "dataset = [x_train, y_train, x_test, y_test]  # pack the dataset for the NetworkManager\n",
    "\n",
    "previous_acc = 0.0\n",
    "total_reward = 0.0\n",
    "\n",
    "with policy_sess.as_default():\n",
    "    # create the Controller and build the internal policy network\n",
    "    controller = Controller(policy_sess, NUM_LAYERS, state_space,\n",
    "                            reg_param=REGULARIZATION,\n",
    "                            exploration=EXPLORATION,\n",
    "                            controller_cells=CONTROLLER_CELLS,\n",
    "                            embedding_dim=EMBEDDING_DIM,\n",
    "                            restore_controller=RESTORE_CONTROLLER)\n",
    "\n",
    "# create the Network Manager\n",
    "manager = NetworkManager(dataset, epochs=MAX_EPOCHS, child_batchsize=CHILD_BATCHSIZE, clip_rewards=CLIP_REWARDS,\n",
    "                         acc_beta=ACCURACY_BETA)\n",
    "\n",
    "# get an initial random state space if controller needs to predict an\n",
    "# action from the initial state\n",
    "state = state_space.get_random_state_space(NUM_LAYERS)\n",
    "print(\"Initial Random State : \", state_space.parse_state_space_list(state))\n",
    "print()\n",
    "\n",
    "# clear the previous files\n",
    "controller.remove_files()\n",
    "\n",
    "# train for number of trails\n",
    "for trial in range(MAX_TRIALS):\n",
    "    with policy_sess.as_default():\n",
    "        K.set_session(policy_sess)\n",
    "        actions = controller.get_action(state)  # get an action for the previous state\n",
    "\n",
    "    # print the action probabilities\n",
    "    state_space.print_actions(actions)\n",
    "    print(\"Predicted actions : \", state_space.parse_state_space_list(actions))\n",
    "\n",
    "    # build a model, train and get reward and accuracy from the network manager\n",
    "    reward, previous_acc = manager.get_rewards(model_fn, state_space.parse_state_space_list(actions))\n",
    "    print(\"Rewards : \", reward, \"Accuracy : \", previous_acc)\n",
    "\n",
    "    with policy_sess.as_default():\n",
    "        K.set_session(policy_sess)\n",
    "\n",
    "        total_reward += reward\n",
    "        print(\"Total reward : \", total_reward)\n",
    "\n",
    "        # actions and states are equivalent, save the state and reward\n",
    "        state = actions\n",
    "        controller.store_rollout(state, reward)\n",
    "\n",
    "        # train the controller on the saved state and the discounted rewards\n",
    "        loss = controller.train_step()\n",
    "        print(\"Trial %d: Controller loss : %0.6f\" % (trial + 1, loss))\n",
    "\n",
    "        # write the results of this trial into a file\n",
    "        with open('train_history.csv', mode='a+') as f:\n",
    "            data = [previous_acc, reward]\n",
    "            data.extend(state_space.parse_state_space_list(state))\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n",
    "    print()\n",
    "\n",
    "print(\"Total Reward : \", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
